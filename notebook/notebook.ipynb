{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe18ff12",
   "metadata": {},
   "source": [
    "saas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "837d5057-5d7a-4660-83bd-a59b9b9ec3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "from sklearn import preprocessing\n",
    "def mfcc_extraction(audio_filename, #.wav filename\n",
    "    hop_duration, #hop_length in seconds, e.g., 0.015s (i.e., 15ms)\n",
    "    num_mfcc #number of mfcc features\n",
    "    ):\n",
    "    speech = AudioSegment.from_wav(audio_filename) #Read audio data from file\n",
    "    samples = speech.get_array_of_samples() #samples x(t)\n",
    "    sampling_rate = speech.frame_rate #sampling rate f\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "    y=np.float32(samples),\n",
    "    sr = sampling_rate,\n",
    "    hop_length = int(sampling_rate * hop_duration),\n",
    "    n_mfcc = num_mfcc)\n",
    "\n",
    "    return mfcc.T\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "def learningGMM(features, #list of feature vectors, each feature vector is an array\n",
    " n_components, #the number of components\n",
    " max_iter #maximum number of iterations\n",
    " ):\n",
    "    gmm = GaussianMixture(n_components = n_components, max_iter = max_iter)\n",
    "    gmm.fit(features)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a62658-ed6e-4d7e-9b09-db35f0d0f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anthony', 'AppleEater', 'Ara', 'Argail', 'Ariyan', 'Arjuan', 'Artem', 'Arthur', 'Artk', 'Arun', 'Arvala', 'Asalkeld', 'Asladic', 'Asp', 'Azmisov', 'B', 'Bachroxx', 'Bae', 'Bahoke', 'Bareford', 'Bart', 'Bassel', 'Beady', 'Beez', 'BelmontGuy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'C:/Users/User/Desktop/Class Folder/Computer Vision/SpeakerData/'\n",
    "speakers = os.listdir(path + 'Train/')\n",
    "print(speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42652d4c-bbad-4fbd-8ba6-6b6a1953476d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#this list is used to store the MFCC features of all training data of all speakers\n",
    "mfcc_all_speakers = []\n",
    "hop_duration = 0.015 #15ms\n",
    "num_mfcc = 12\n",
    "for s in speakers:\n",
    "    sub_path = path + 'Train/' + s + '/'\n",
    "    sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "    mfcc_one_speaker = np.asarray(())\n",
    "    for fn in sub_file_names:\n",
    "        mfcc_one_file = mfcc_extraction(fn, hop_duration, num_mfcc)\n",
    "        if mfcc_one_speaker.size == 0:\n",
    "            mfcc_one_speaker = mfcc_one_file\n",
    "        else:\n",
    "            mfcc_one_speaker = np.vstack((mfcc_one_speaker, mfcc_one_file))\n",
    "    mfcc_all_speakers.append(mfcc_one_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad16d13-d25a-4af4-bc4e-a3e701aa41f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "for i in range(0, len(speakers)):\n",
    "    with open('C:/Users/User/Desktop/Class Folder/Computer Vision/TrainingFeatures/' + speakers[i] + '_mfcc.fea','wb') as f:\n",
    "        pickle.dump(mfcc_all_speakers[i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7803c4c-7ac5-4043-8277-7c70565b5662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "max_iter = 50\n",
    "gmms = [] #list of GMMs, each is for a speaker\n",
    "for i in range(0, len(speakers)):\n",
    "    gmm = learningGMM(mfcc_all_speakers[i],n_components,max_iter)\n",
    "    gmms.append(gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2b0272-01b3-494b-b4f6-6b95e327b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(speakers)):\n",
    "    with open('Models/' + speakers[i] + '.gmm', 'wb') as f: #'wb' is for binary write\n",
    "        pickle.dump(gmms[i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c49ea7-c27b-4c87-a470-23cff38fafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmms = []\n",
    "for i in range(len(speakers)):\n",
    "    with open('Models/' + speakers[i] + '.gmm', 'rb') as f: #'wb' is for binary write\n",
    "        gmm = pickle.load(f)\n",
    "        gmms.append(gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3946a352-0a34-481c-b2b9-8ab13b8643e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the speaker recognition method\n",
    "def speaker_recognition(audio_file_name, gmms):\n",
    "    # Extract MFCC features from the input audio file\n",
    "    mfcc_features = mfcc_extraction(audio_file_name, hop_duration, num_mfcc)\n",
    "\n",
    "    # Calculate the likelihood scores for each speaker's GMM\n",
    "    likelihood_scores = [gmm.score(mfcc_features) for gmm in gmms]\n",
    "\n",
    "    # Find the speaker ID with the highest likelihood score\n",
    "    speaker_id = np.argmax(likelihood_scores)\n",
    "\n",
    "    return speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3722ec3a-8438-4ef8-929f-69d210134d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ara\n"
     ]
    }
   ],
   "source": [
    "speaker_id = speaker_recognition('SpeakerData/Test/Ara/a0522.wav', gmms)\n",
    "print(speakers[speaker_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1216493-f8ce-47c7-961c-91841bbfcdff",
   "metadata": {},
   "source": [
    "#### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c7adff-6315-4346-b1bf-5bbf403c9c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition Accuracy: 92.57%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import librosa\n",
    "\n",
    "# Test the speaker recognition algorithm on the entire test set\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Iterate through test files\n",
    "for root, _, files in os.walk('SpeakerData/Test'):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            total_samples += 1\n",
    "            audio_file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Identify the speaker using the recognition method\n",
    "            predicted_speaker_id = speaker_recognition(audio_file_path, gmms)\n",
    "            \n",
    "            # Get the true speaker ID from the folder name (assuming the folder name is the speaker's name)\n",
    "            true_speaker = os.path.basename(root)\n",
    "            true_speaker_id = speakers.index(true_speaker)\n",
    "            \n",
    "            # Check if the prediction is correct\n",
    "            if predicted_speaker_id == true_speaker_id:\n",
    "                correct_predictions += 1\n",
    "\n",
    "# Calculate and print the recognition accuracy\n",
    "accuracy = (correct_predictions / total_samples) * 100\n",
    "print(f\"Recognition Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
